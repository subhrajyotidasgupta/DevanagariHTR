{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['repository']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization\nimport matplotlib.pyplot as plt\nimport numpy as np","execution_count":2,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"(train_x, train_y),(test_x, test_y) = mnist.load_data()","execution_count":3,"outputs":[{"output_type":"stream","text":"Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)\ntest_x = test_x.reshape(test_x.shape[0], 28, 28, 1)\ntrain_x = train_x/255.0\ntest_x = test_x/255.0","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3,3), input_shape = (28, 28, 1), name='conv_1'))\nmodel.add(BatchNormalization(name='batch_norm_1'))\n\nmodel.add(Conv2D(64, (3,3), name='conv_2'))\nmodel.add(BatchNormalization(name='batch_norm_2'))\nmodel.add(MaxPooling2D(pool_size=(2,2), name='max_pool_1'))\n\nmodel.add(Flatten(name='flatten'))\nmodel.add(Dense(128, activation='relu', name='dense_1'))\nmodel.add(BatchNormalization(name='batch_norm_3'))\n\nmodel.add(Dense(10, activation='softmax', name='dense_2'))\n\nmodel.compile(loss= 'categorical_crossentropy' , optimizer= keras.optimizers.Adadelta(), metrics=['accuracy'])","execution_count":5,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = model.fit(train_x, keras.utils.to_categorical(train_y), batch_size=128, epochs=5, validation_split=0.2)","execution_count":6,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nTrain on 48000 samples, validate on 12000 samples\nEpoch 1/5\n48000/48000 [==============================] - 7s 156us/step - loss: 0.1595 - acc: 0.9533 - val_loss: 0.0779 - val_acc: 0.9760\nEpoch 2/5\n48000/48000 [==============================] - 4s 81us/step - loss: 0.0529 - acc: 0.9849 - val_loss: 0.0654 - val_acc: 0.9803\nEpoch 3/5\n48000/48000 [==============================] - 4s 80us/step - loss: 0.0290 - acc: 0.9921 - val_loss: 0.0504 - val_acc: 0.9841\nEpoch 4/5\n48000/48000 [==============================] - 4s 80us/step - loss: 0.0152 - acc: 0.9965 - val_loss: 0.0556 - val_acc: 0.9832\nEpoch 5/5\n48000/48000 [==============================] - 4s 81us/step - loss: 0.0079 - acc: 0.9986 - val_loss: 0.0475 - val_acc: 0.9861\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights('weights.h5')","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model = Sequential()\n\nnew_model.add(Conv2D(32, kernel_size=(3,3), input_shape = (28, 28, 1), name='conv_1', trainable= False))\nnew_model.add(BatchNormalization(name='batch_norm_1', trainable= False))\n\nnew_model.add(Conv2D(64, (3,3), name='conv_2', trainable= False))\nnew_model.add(BatchNormalization(name='batch_norm_2', trainable= False))\nnew_model.add(MaxPooling2D(pool_size=(2,2), name='max_pool_1', trainable= False))\n\nnew_model.add(Flatten())\nnew_model.add(Dense(128, activation='relu', name='dense_1', trainable= False))\nnew_model.add(BatchNormalization(name='batch_norm_3', trainable= False))\n\nnew_model.add(Dense(10, activation='softmax', name='dense_2'))\n\nnew_model.compile(loss= 'categorical_crossentropy' , optimizer= keras.optimizers.Adadelta(), metrics=['accuracy'])","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model.load_weights('weights.h5', by_name=True)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(train_x[9].reshape(28, 28),cmap='gray', vmin=0, vmax=1 )","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f8b0017b9e8>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_img(img):\n    image = cv2.copyMakeBorder(img.reshape(28,28),5,5,5,5,cv2.BORDER_CONSTANT,value=[255,255,255])\n    image = cv2.resize(image, (28, 28))\n    ret,image = cv2.threshold(image,0.85,1,cv2.THRESH_BINARY_INV) \n    image = np.expand_dims(image, 0)\n    image = np.expand_dims(image, -1)\n    return image","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nfrom keras.preprocessing.image import ImageDataGenerator\n\ndef preprocess_img(img):\n    image = cv2.copyMakeBorder(img.reshape(28,28),5,5,5,5,cv2.BORDER_CONSTANT,value=[255,255,255])\n    image = cv2.resize(image, (28, 28))\n    ret,image = cv2.threshold(image,0.85,1,cv2.THRESH_BINARY_INV) \n    image = np.expand_dims(image, 0)\n    image = np.expand_dims(image, -1)\n    return image\n\ndatagen = ImageDataGenerator(\n    rescale = 1. /255,\n    data_format = 'channels_last',\n    preprocessing_function = preprocess_img #this statement perhaps could be creating the anomaly)\n\ntrain_generator = datagen.flow_from_directory('../input/repository/subhrajyotidasgupta-barc_projects-4b267ca/Numeral/train',\n                                                   target_size = (28,28),\n                                                   batch_size = 8,\n                                                   color_mode = 'grayscale',\n                                                   class_mode = 'categorical')\n\nvalid_generator = datagen.flow_from_directory('../input/repository/subhrajyotidasgupta-barc_projects-4b267ca/Numeral/valid',\n                                                   target_size = (28,28),\n                                                   batch_size = 8,\n                                                   color_mode = 'grayscale',\n                                                   class_mode = 'categorical')","execution_count":12,"outputs":[{"output_type":"stream","text":"Found 106 images belonging to 10 classes.\nFound 28 images belonging to 10 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(train_generator[0][0][0].reshape(28,28))\n#preprocess_img(valid_generator[0][0][0]).shape\n#train_generator[0][0][0].shape\n#plt.imshow(preprocess_img(train_generator[0][0][0]).reshape(28,28))\n#print(train_generator[0][0][0].reshape(28,28))","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f86d002ad68>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAACnBJREFUeJzt3UGInPd5x/Hvr5YsUyUHuWmFcEyTBlMwhShlUQsxJcVNcHyRcwnRIahgUA4xJJBDTHqoj6Y0CT2UgFKLqCV1KCTGOpg2qgiYQDFeG9eW7bZyjUIkZKnBhziFyrLz9LCvw8be1a533pl3zPP9wDAz77y778Ogr2bmnYV/qgpJ/fzG1ANImobxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9TUrkUe7MbsqZvYu8hDSq38H//L63U129l3pviT3AX8DXAD8HdV9eD19r+JvfxR7pzlkJKu44k6s+19d/y2P8kNwN8CnwZuB44kuX2nv0/SYs3ymf8Q8FJVvVxVrwPfAw6PM5akeZsl/luAn667f2HY9muSHEuymmT1GldnOJykMc39bH9VHa+qlapa2c2eeR9O0jbNEv9F4NZ19z84bJP0HjBL/E8CtyX5cJIbgc8Bp8YZS9K87firvqp6I8l9wL+w9lXfiap6frTJJM3VTN/zV9VjwGMjzSJpgfzzXqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qamZVulNch54DXgTeKOqVsYYStL8zRT/4E+r6mcj/B5JC+TbfqmpWeMv4IdJnkpybIyBJC3GrG/776iqi0l+Bzid5D+q6vH1Owz/KRwDuInfnPFwksYy0yt/VV0crq8AjwCHNtjneFWtVNXKbvbMcjhJI9px/En2Jnn/W7eBTwFnxxpM0nzN8rZ/P/BIkrd+zz9W1T+PMpWkudtx/FX1MvDREWeRtEB+1Sc1ZfxSU8YvNWX8UlPGLzVl/FJTxi81ZfxSU8YvNWX8UlPGLzVl/FJTxi81ZfxSU8YvNWX8UlPGLzVl/FJTxi81ZfxSU8YvNWX8UlPGLzVl/FJTxi81ZfxSU8YvNWX8UlPGLzVl/FJTW8af5ESSK0nOrtt2c5LTSc4N1/vmO6aksW3nlf87wF1v23Y/cKaqbgPODPclvYdsGX9VPQ68+rbNh4GTw+2TwD0jzyVpznb6mX9/VV0abr8C7B9pHkkLMvMJv6oqoDZ7PMmxJKtJVq9xddbDSRrJTuO/nOQAwHB9ZbMdq+p4Va1U1cpu9uzwcJLGttP4TwFHh9tHgUfHGUfSomznq76HgX8Dfj/JhST3Ag8Cn0xyDviz4b6k95BdW+1QVUc2eejOkWeRtED+hZ/UlPFLTRm/1JTxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtNbRl/khNJriQ5u27bA0kuJnlmuNw93zEljW07r/zfAe7aYPs3q+rgcHls3LEkzduW8VfV48CrC5hF0gLN8pn/viTPDh8L9o02kaSF2Gn83wI+AhwELgFf32zHJMeSrCZZvcbVHR5O0th2FH9VXa6qN6vql8C3gUPX2fd4Va1U1cpu9ux0Tkkj21H8SQ6su/sZ4Oxm+0paTru22iHJw8AngA8kuQD8JfCJJAeBAs4DX5jjjJLmYMv4q+rIBpsfmsMskhbIv/CTmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pqS3jT3Jrkh8leSHJ80m+NGy/OcnpJOeG633zH1fSWLbzyv8G8JWquh34Y+CLSW4H7gfOVNVtwJnhvqT3iC3jr6pLVfX0cPs14EXgFuAwcHLY7SRwz7yGlDS+d/WZP8mHgI8BTwD7q+rS8NArwP5RJ5M0V9uOP8n7gO8DX66qn69/rKoKqE1+7liS1SSr17g607CSxrOt+JPsZi3871bVD4bNl5McGB4/AFzZ6Ger6nhVrVTVym72jDGzpBFs52x/gIeAF6vqG+seOgUcHW4fBR4dfzxJ87JrG/t8HPg88FySZ4ZtXwMeBP4pyb3AT4DPzmdESfOwZfxV9WMgmzx857jjSFoU/8JPasr4paaMX2rK+KWmjF9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpraMP8mtSX6U5IUkzyf50rD9gSQXkzwzXO6e/7iSxrJrG/u8AXylqp5O8n7gqSSnh8e+WVV/Pb/xJM3LlvFX1SXg0nD7tSQvArfMezBJ8/WuPvMn+RDwMeCJYdN9SZ5NciLJvk1+5liS1SSr17g607CSxrPt+JO8D/g+8OWq+jnwLeAjwEHW3hl8faOfq6rjVbVSVSu72TPCyJLGsK34k+xmLfzvVtUPAKrqclW9WVW/BL4NHJrfmJLGtp2z/QEeAl6sqm+s235g3W6fAc6OP56kednO2f6PA58HnkvyzLDta8CRJAeBAs4DX5jLhJLmYjtn+38MZIOHHht/HEmL4l/4SU0Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9RUqmpxB0v+B/jJuk0fAH62sAHenWWdbVnnAmfbqTFn+92q+u3t7LjQ+N9x8GS1qlYmG+A6lnW2ZZ0LnG2npprNt/1SU8YvNTV1/McnPv71LOtsyzoXONtOTTLbpJ/5JU1n6ld+SROZJP4kdyX5zyQvJbl/ihk2k+R8kueGlYdXJ57lRJIrSc6u23ZzktNJzg3XGy6TNtFsS7Fy83VWlp70uVu2Fa8X/rY/yQ3AfwGfBC4ATwJHquqFhQ6yiSTngZWqmvw74SR/AvwC+Puq+oNh218Br1bVg8N/nPuq6qtLMtsDwC+mXrl5WFDmwPqVpYF7gD9nwufuOnN9lgmetyle+Q8BL1XVy1X1OvA94PAEcyy9qnocePVtmw8DJ4fbJ1n7x7Nwm8y2FKrqUlU9Pdx+DXhrZelJn7vrzDWJKeK/BfjpuvsXWK4lvwv4YZKnkhybepgN7B+WTQd4Bdg/5TAb2HLl5kV628rSS/Pc7WTF67F5wu+d7qiqPwQ+DXxxeHu7lGrtM9syfV2zrZWbF2WDlaV/ZcrnbqcrXo9tivgvAreuu//BYdtSqKqLw/UV4BGWb/Xhy28tkjpcX5l4nl9ZppWbN1pZmiV47pZpxesp4n8SuC3Jh5PcCHwOODXBHO+QZO9wIoYke4FPsXyrD58Cjg63jwKPTjjLr1mWlZs3W1maiZ+7pVvxuqoWfgHuZu2M/38DfzHFDJvM9XvAvw+X56eeDXiYtbeB11g7N3Iv8FvAGeAc8K/AzUs02z8AzwHPshbagYlmu4O1t/TPAs8Ml7unfu6uM9ckz5t/4Sc15Qk/qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5r6fw80T/MeHrUgAAAAAElFTkSuQmCC\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model.fit_generator(train_generator,\n                       validation_data = valid_generator,\n                       validation_steps = 4096,\n                       epochs = 10,\n                       steps_per_epoch = 2)","execution_count":20,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n2/2 [==============================] - 30s 15s/step - loss: 2.2410 - acc: 0.1250 - val_loss: 11.7434 - val_acc: 0.1072\nEpoch 2/10\n2/2 [==============================] - 31s 15s/step - loss: 2.3494 - acc: 0.0625 - val_loss: 11.6997 - val_acc: 0.1071\nEpoch 3/10\n2/2 [==============================] - 31s 15s/step - loss: 2.2283 - acc: 0.0625 - val_loss: 11.7184 - val_acc: 0.1072\nEpoch 4/10\n2/2 [==============================] - 31s 16s/step - loss: 2.2768 - acc: 0.2750 - val_loss: 11.6412 - val_acc: 0.1071\nEpoch 5/10\n2/2 [==============================] - 31s 15s/step - loss: 2.3581 - acc: 0.1250 - val_loss: 11.6358 - val_acc: 0.1072\nEpoch 6/10\n1/2 [==============>...............] - ETA: 0s - loss: 2.3360 - acc: 0.0000e+00","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-eeb19e87bec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                        \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                        \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                        steps_per_epoch = 2)\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    232\u001b[0m                             \u001b[0mval_enqueuer_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             workers=0)\n\u001b[0m\u001b[1;32m    235\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                         \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1470\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    344\u001b[0m                                  \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                                  str(generator_output))\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mouts_per_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1254\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2 \n\nfig = plt.figure(figsize=(20,20))\nfor i in range(1,11):\n    img = cv2.copyMakeBorder(train_generator[i][0][0].reshape(28,28),5,5,5,5,cv2.BORDER_CONSTANT,value=[255,255,255])\n    img = cv2.resize(img, (28, 28))\n    ret,img = cv2.threshold(img,0.85,1,cv2.THRESH_BINARY_INV) \n    #kernel = np.ones((2,2), np.uint8) \n    #img = cv2.dilate(img, kernel, iterations=1)\n    \n    fig.add_subplot(7,5,i)\n    plt.imshow(img, cmap='gray', vmin=0, vmax=1)\n    #print(img)\n    img = np.expand_dims(img, 0)\n    img = np.expand_dims(img, -1)\n\n    \n    train_preds = new_model.predict(img)\n    plt.title(\"Prediction = \" + str(np.argmax(train_preds)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}